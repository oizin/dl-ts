{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian LSTM via dropout\n",
    "\n",
    "Some description\n",
    "\n",
    "To do:\n",
    "\n",
    "Add in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from energy_data import import_energy_data\n",
    "from ts_utils import create_sliding_window\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from models.lstm_encoder_decoder import lstm_seq2seq\n",
    "from models.bayesian_lstm_dropout import BayesLSTMencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electricity data\n",
    "\n",
    "We'll need two sets of data, for the training and pretraining steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   date  day_of_week  hour_of_day  \\\n",
      "date                                                                \n",
      "2016-01-11 17:00:00 2016-01-11 17:00:00          0.0         17.0   \n",
      "2016-01-11 18:00:00 2016-01-11 18:00:00          0.0         18.0   \n",
      "2016-01-11 19:00:00 2016-01-11 19:00:00          0.0         19.0   \n",
      "2016-01-11 20:00:00 2016-01-11 20:00:00          0.0         20.0   \n",
      "2016-01-11 21:00:00 2016-01-11 21:00:00          0.0         21.0   \n",
      "\n",
      "                     log_energy_consumption  \n",
      "date                                         \n",
      "2016-01-11 17:00:00                4.007333  \n",
      "2016-01-11 18:00:00                5.174265  \n",
      "2016-01-11 19:00:00                5.155217  \n",
      "2016-01-11 20:00:00                4.828314  \n",
      "2016-01-11 21:00:00                4.637960  \n"
     ]
    }
   ],
   "source": [
    "# we'll use the electricity data for this, import:\n",
    "energy_df = import_energy_data()\n",
    "print(energy_df.head())\n",
    "\n",
    "train_split = 0.7\n",
    "n_train = int(train_split * len(energy_df))\n",
    "n_test = len(energy_df) - n_train\n",
    "\n",
    "features = ['log_energy_consumption','day_of_week','hour_of_day']\n",
    "feature_array = energy_df[features].values\n",
    "\n",
    "# Fit Scaler only on Training features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_array[:n_train])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "target_scaler.fit(feature_array[:n_train,0:1])\n",
    "\n",
    "def inverse_transform(x):\n",
    "    return np.exp(target_scaler.inverse_transform(x.reshape(-1, 1)))\n",
    "\n",
    "# Transform on both Training and Test data\n",
    "scaled_array = pd.DataFrame(scaler.transform(feature_array),\n",
    "                            columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log_energy_consumption  day_of_week  hour_of_day\n",
      "0               -0.535884      -1.5089     0.794729\n",
      "1                1.287582      -1.5089     0.939174\n",
      "2                1.257817      -1.5089     1.083619\n",
      "3                0.746993      -1.5089     1.228064\n",
      "4                0.449544      -1.5089     1.372509\n"
     ]
    }
   ],
   "source": [
    "print(scaled_array.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train LSTM encoder decoder -> train LSTM\n",
    "\n",
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityDatasetED(Dataset):\n",
    "    \n",
    "    def __init__(self,X,y):\n",
    "        self.X,self.y = X,y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].to(DEVICE),self.y[idx].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining data: only the time series\n",
    "sequence_length = 24\n",
    "output_size = 12\n",
    "Xe, ye = create_sliding_window(pd.DataFrame(scaled_array.loc[:,'log_energy_consumption']), sequence_length,output_size=output_size)\n",
    "\n",
    "Xe_train = Xe[:n_train]\n",
    "ye_train = ye[:n_train]\n",
    "\n",
    "Xe_test = Xe[n_train:]\n",
    "ye_test = ye[n_train:]\n",
    "\n",
    "Xe_train = torch.tensor(Xe_train).float()\n",
    "ye_train = torch.tensor(ye_train).unsqueeze(2).float()\n",
    "Xe_test = torch.tensor(Xe_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model,dl_train,dl_valid,n_epochs,optimizer, loss_fn):\n",
    "    training_losses = np.full(n_epochs, np.nan)\n",
    "    eval_losses = np.full(n_epochs, np.nan)\n",
    "    with tqdm(total=n_epochs) as tr:\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = model.train_single_epoch(dl_train, optimizer, loss_fn)\n",
    "            eval_loss = model.evaluate(dl_valid, loss_fn)\n",
    "            # loss for epoch \n",
    "            training_losses\n",
    "            training_losses[epoch] = train_loss\n",
    "            eval_losses[epoch] = eval_loss\n",
    "            # progress bar \n",
    "            tr.set_postfix(eval_loss=\"{0:.5f}\".format(eval_loss))\n",
    "            tr.update()\n",
    "    return training_losses,eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ElectricityDatasetED(Xe_train, ye_train),batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2/30 [00:15<03:41,  7.90s/it, epoch_loss=0.18235]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f6b8c43e9976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{0:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dl-ts/models/lstm_encoder_decoder.py\u001b[0m in \u001b[0;36mtrain_single_epoch\u001b[0;34m(self, dataloader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# loss for epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "learning_rate = 0.01\n",
    "hidden_size = 32\n",
    "loss_fn = nn.MSELoss()\n",
    "encoder_decoder = lstm_seq2seq(1, hidden_size).to(DEVICE)\n",
    "optimizer = optim.Adam(encoder_decoder.parameters(), lr = learning_rate)\n",
    "with tqdm(total=n_epochs) as tr:\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = encoder_decoder.train_single_epoch(dl_train, optimizer, loss_fn)\n",
    "        tr.set_postfix(epoch_loss=\"{0:.5f}\".format(epoch_loss))\n",
    "        tr.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectricityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,X_encode,X_other,y):\n",
    "        self.X_encode,self.X_other,self.y = X_encode,X_other,y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_encode[idx].to(DEVICE),self.X_other[idx].to(DEVICE),self.y[idx].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data: time series plut other stuff\n",
    "Xo, y = scaled_array.loc[:,['day_of_week','hour_of_day']].values, scaled_array.loc[:,['log_energy_consumption']].values\n",
    "\n",
    "Xo_train = Xo[:n_train]\n",
    "y_train = y[:n_train]\n",
    "Xo_train = Xo_train[sequence_length:(-output_size)]\n",
    "y_train = y_train[sequence_length:(-output_size)]\n",
    "\n",
    "Xo_test = Xo[n_train:]\n",
    "y_test = y[n_train:]\n",
    "Xo_test = Xo_test[sequence_length:(-output_size)]\n",
    "y_test = y_test[sequence_length:(-output_size)]\n",
    "\n",
    "Xo_train = torch.tensor(Xo_train).unsqueeze(1).float()\n",
    "y_train = torch.tensor(y_train).unsqueeze(2).float()\n",
    "Xo_test = torch.tensor(Xo_test).unsqueeze(1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder_bool(model, train = False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad & name.startswith(\"encoder\"):\n",
    "            param.requires_grad = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 2\n",
    "tscv = TimeSeriesSplit(n_splits=nfolds)\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 30\n",
    "hidden_size = 32\n",
    "loss_fn = nn.MSELoss()\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(Xe_train)):\n",
    "    # prep data\n",
    "    Xe_tr, Xo_tr, y_tr = Xe_train[torch.LongTensor(train_index)],Xo_train[torch.LongTensor(train_index)],y_train[torch.LongTensor(train_index)]\n",
    "    Xe_v, Xo_v, y_v = Xe_train[torch.LongTensor(test_index)],Xo_train[torch.LongTensor(test_index)],y_train[torch.LongTensor(test_index)]\n",
    "    dl_train = DataLoader(ElectricityDataset(Xe_tr, Xo_tr, y_tr),batch_size=5)\n",
    "    dl_valid = DataLoader(ElectricityDataset(Xe_v, Xo_v, y_v),batch_size=1)\n",
    "    # model\n",
    "    model = BayesLSTMencoder(encoder_decoder.encoder,2,hidden_size,1).to(DEVICE)\n",
    "    train_encoder_bool(model,False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    training_losses,eval_losses = train_and_evaluate(model,dl_train,dl_valid,n_epochs,optimizer, loss_fn)\n",
    "    plt.plot(training_losses,label=\"training\",color=\"blue\")\n",
    "    plt.plot(eval_losses,label=\"validation\",color=\"red\")\n",
    "    if fold == 0:\n",
    "        plt.legend()\n",
    "    break # only once for time\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ElectricityDataset(Xe_train,Xo_train,y_train),batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "hidden_size = 16\n",
    "model = BayesLSTMencoder(encoder_decoder.encoder,2,hidden_size,1).to(DEVICE)\n",
    "train_encoder_bool(model,False)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "with tqdm(total=n_epochs) as tr:\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = model.train_single_epoch(dl_train, optimizer, loss_fn)\n",
    "        tr.set_postfix(epoch_loss=\"{0:.5f}\".format(epoch_loss))\n",
    "        tr.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Predict on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference - mean only\n",
    "y_preds = []\n",
    "for xe,xo in zip(Xe_test,Xo_test):\n",
    "    y_ = model.inference(xe.to(DEVICE),xo.to(DEVICE))\n",
    "    y_preds.append(y_.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.array(y_preds)\n",
    "y_tests = np.concatenate(y_test)\n",
    "print(\"RMSE \",np.sqrt(np.mean((y_preds-y_tests)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE (original scale)\",np.sqrt(np.mean((inverse_transform(y_preds)-inverse_transform(y_tests))**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(np.min(y_preds),np.max(y_preds)),np.linspace(np.min(y_tests),np.max(y_tests)),\"r--\")\n",
    "plt.scatter(y_preds,y_tests)\n",
    "plt.xlabel(\"predictions\")\n",
    "plt.ylabel(\"truth\")\n",
    "plt.title(\"Bayesian LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example predictions\n",
    "plt.plot(inverse_transform(y_tests[0:72]))\n",
    "plt.plot(inverse_transform(y_preds[0:72]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference - mean plus sd\n",
    "def prediction_samples(Xe_test,Xo_test,B):\n",
    "    y_preds_B = np.full((B,Xe_test.size(0)),np.nan)\n",
    "    with tqdm(total=B) as tr:\n",
    "        for b in range(B):\n",
    "            y_preds = []\n",
    "            for xe,xo in zip(Xe_test,Xo_test):\n",
    "                y_ = model.inference(xe.to(DEVICE),xo.to(DEVICE),dropout=0.25)\n",
    "                y_preds.append(y_.squeeze())\n",
    "            y_preds_B[b,:] = np.array(y_preds)\n",
    "            tr.update()\n",
    "    return(y_preds_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prediction intervals\n",
    "B = 100\n",
    "y_preds_sample = prediction_samples(Xe_test,Xo_test,B)\n",
    "plt.plot(inverse_transform(y_tests[0:72]))\n",
    "for b in range(B):\n",
    "    plt.plot(inverse_transform(y_preds_sample[b,0:72]),\"r-\",alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
